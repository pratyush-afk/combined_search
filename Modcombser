import fastapi
import os
import asyncio
from fastapi import Header, HTTPException
from pydantic import BaseModel
from typing import Optional, Any, List, Dict
from collections import deque, defaultdict
from google.genai import types
from utilities.custom_logging import log as logger
from fastapi.responses import HTMLResponse

# --- Imports from your specific project structure ---
try:
    from config import Config
    from subsystem.Document_Enhancement import DocumentEnhancement
    from routers.knowledgeSearch import KnowledgeSearchService
except ImportError as e:
    logger.critical(f"Failed to import services: {e}. Ensure service files are accessible.")
    raise

# --- 1. Global In-Memory History Storage (FIFO - Last 10) ---
# Maps userid -> deque of strings. maxlen=10 ensures the 11th item pushes out the 1st.
user_search_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10))

# --- 2. Pydantic Models ---
class HybridSearchQuery(BaseModel):
    """Model for hybrid search query input."""
    query: str
    domain: Optional[str] = None

class HybridSearchResponse(BaseModel):
    """Model for hybrid search response."""
    combined_result: str
    qachain_response: str
    document_search_response: str

class HistoryResponse(BaseModel):
    """Model for viewing user history."""
    userid: str
    recent_questions: List[str]

# --- 3. Router Setup ---
def get_router() -> fastapi.APIRouter:
    """
    Create a router for hybrid search that also manages user history.
    Returns:
        fastapi.APIRouter
    """
    router = fastapi.APIRouter()

    # --- Initialize Services ---
    try:
        cnf = Config()
        project_id = cnf.agent.project
        
        knowledge_search_service = KnowledgeSearchService(project_id)
        doc_search_service = DocumentEnhancement(project_id, config=cnf)
        
        logger.info("Initialized KnowledgeSearchService and DocumentEnhancement.")
    except Exception as e:
        logger.critical(f"Failed to initialize services at startup: {e}")
        knowledge_search_service = None
        doc_search_service = None

    # --- 4. Main Hybrid Search Endpoint ---
    @router.post("/hybrid_search", response_model=HybridSearchResponse)
    async def hybrid_search(
        request_body: HybridSearchQuery, 
        request: fastapi.Request,
        userid: str = Header(...)  # Required header: userid
    ):
        """
        1. Stores query in History (FIFO).
        2. Runs QA Chain and Document Search in parallel.
        3. Synthesizes result using LLM.
        """
        logger.info(f"Hybrid search query: {request_body.query} for userid: {userid}")

        # --- A. Update History (FIFO) ---
        try:
            user_search_history[userid].append(request_body.query)
            logger.info(f"History updated for {userid}. Current Queue: {len(user_search_history[userid])}")
        except Exception as history_e:
            logger.warning(f"Failed to update history log (non-critical): {history_e}")

        # --- B. Service Check ---
        if not knowledge_search_service or not doc_search_service:
            logger.error("Search services are not initialized.")
            raise HTTPException(status_code=503, detail="Search services are unavailable.")
            
        try:
            logger.info("Running QA chain and Document Search in parallel...")
            
            # Create async tasks
            qa_task = knowledge_search_service.qa_chain_response(request_body.query)
            # This returns HTMLResponse object
            doc_task = doc_search_service.invoke_chain(request_body.query, userID=userid) 
            
            # Execute concurrently
            results = await asyncio.gather(
                qa_task,
                doc_task,
                return_exceptions=True
            )

            # --- C. Process QA Chain Result ---
            if isinstance(results[0], Exception):
                logger.error(f"QA chain task failed: {results[0]}")
                qachain_result = "Error: QA chain failed to produce a result."
            else:
                qachain_result = str(results[0])

            # --- D. Process Document Search Result (Handle HTML) ---
            if isinstance(results[1], Exception):
                logger.error(f"Document search task failed: {results[1]}")
                doc_search_result = "Error: Document search failed to produce a result."
            
            elif isinstance(results[1], HTMLResponse):
                logger.info("Document search returned HTML. Preserving formatted content.")
                try:
                    # Extract HTML body directly
                    html_body = results[1].body.decode("utf-8")
                    doc_search_result = html_body
                except Exception as e:
                    logger.error(f"Failed to decode HTMLResponse body: {e}")
                    doc_search_result = "Document search returned unparseable HTML."
            else:
                # Fallback for non-HTML objects
                doc_search_result = str(getattr(results[1], 'answer', results[1]))
            
            logger.info(f"QA Chain Response: {qachain_result}")
            # Log only snippet of HTML to avoid clutter
            logger.info(f"Document Search Response (Raw HTML snippet): {doc_search_result[:150]}...")
            
            # --- E. Combine with LLM ---
            combined_result = await combine_responses_with_llm(
                request_body.query,
                qachain_result, 
                doc_search_result 
            )
            
            return HybridSearchResponse(
                combined_result=combined_result,
                qachain_response=qachain_result,
                document_search_response=doc_search_result
            )
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    # --- 5. History Management Endpoints ---
    
    @router.get("/history/get", response_model=HistoryResponse)
    async def get_history(userid: str = Header(...)):
        """Retrieve last 10 searches for the userid."""
        try:
            # Convert deque to list (Order: Oldest -> Newest)
            # Use list(reversed(...)) if you want Newest -> Oldest
            return HistoryResponse(
                userid=userid,
                recent_questions=list(user_search_history[userid])
            )
        except Exception as e:
            logger.error(f"Error fetching history: {e}")
            raise HTTPException(status_code=500, detail="Error fetching history")

    @router.delete("/history/clear")
    async def clear_history(userid: str = Header(...)):
        """Clear search history for the userid."""
        if userid in user_search_history:
            user_search_history[userid].clear()
            return {"message": f"History cleared for {userid}"}
        return {"message": "User history empty or not found"}

    # --- 6. Helper Function: LLM Synthesis ---
    async def combine_responses_with_llm(query: str, qachain_response: str, doc_search_response: str) -> str:
        """
        Use LLM to combine and synthesize responses from both sources.
        """
        prompt = f"""
You are a technical support AI assistant specialized in synthesizing information from multiple sources.

User Query: {query}

Source 1 - Conversational AI Agent:
{qachain_response}

Source 2 - Knowledge Base Documents:
{doc_search_response}

Instructions:
1. Provide a concise, actionable answer (3-5 sentences maximum)
2. Merge complementary information from both sources
3. If sources conflict, prefer the most recent or authoritative information
4. Use bullet points only for step-by-step procedures
5. Avoid redundant explanations
6. Focus on practical solutions
7. If one source is significantly better, use that as primary and supplement with the other
8. **IMPORTANT**: If Source 2 contains HTML links (e.g., `<a href=...`), preserve them in your final answer.

Format your response as:
- Direct answer to the query
- Key action items (if applicable)
- Brief explanation (if needed)

Provide ONLY the synthesized response without meta-commentary:
"""
        
        try:
            from google.genai import Client
            from google.cloud import aiplatform
            
            # Ensure these env vars are set or defaults are correct for your Sabre setup
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "sab-dev-gen-ai-tech-6095")
            location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
            model_name = os.getenv("GOOGLE_GEMINI_MODEL", "gemini-2.0-flash-exp")
            
            aiplatform.init(project=project_id, location=location)
            client = Client(vertexai=True, project=project_id, location=location)
            
            response = await client.aio.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    top_p=0.9,
                    max_output_tokens=8000,
                )
            )
            
            combined_response = response.text.strip()
            logger.info(f"Combined LLM Response: {combined_response}")
            return combined_response
            
        except Exception as e:
            logger.error(f"Error combining responses with LLM: {str(e)}")
            # Fallback if LLM fails
            return f"**Quick Answer:** {qachain_response[:200]}...\n\n**Additional Context:** {doc_search_response[:200]}..."

    return router
