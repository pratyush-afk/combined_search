import fastapi
import os
import asyncio  # <-- Added for parallel calls
from pydantic import BaseModel
from typing import Optional, Any
from google.genai import types
from utilities.custom_logging import log as logger
from fastapi.responses import HTMLResponse  # <-- Added from File 1

# --- Added imports from your previous files ---
# (Make sure these paths are correct for your project)
try:
    from config import Config
    from subsystem.Document_Enhancement import DocumentEnhancement
    # Assuming File 2's code is available as 'KnowledgeSearchService'
    # You might need to change 'knowledge_search_service' to the correct
    # file name, e.g., 'from your_file_2 import KnowledgeSearchService'
    from knowledge_search_service import KnowledgeSearchService
except ImportError as e:
    logger.critical(f"Failed to import services: {e}. Ensure service files are accessible.")
    # This will raise an error on startup if imports fail
    raise


class HybridSearchQuery(BaseModel):
    """Model for hybrid search query input."""
    query: str
    domain: Optional[str] = None


class HybridSearchResponse(BaseModel):
    """Model for hybrid search response."""
    combined_result: str
    # --- This structure is kept exactly as you wrote it ---
    qachain_response: str
    document_search_response: str


def get_router() -> fastapi.APIRouter:
    """
    Create a router for hybrid search combining QA chain and document search.

    Returns:
        fastapi.APIRouter : Router for /hybrid_search
    """
    router = fastapi.APIRouter()

    # --- Added: Initialize the services from your previous files ---
    try:
        cnf = Config()
        project_id = cnf.agent.project
        
        # Instantiate the services
        knowledge_search_service = KnowledgeSearchService(project_id)
        doc_search_service = DocumentEnhancement(project_id, config=cnf)
        
        logger.info("Initialized KnowledgeSearchService and DocumentEnhancement.")
    except Exception as e:
        logger.critical(f"Failed to initialize services at startup: {e}")
        knowledge_search_service = None
        doc_search_service = None
    # --- End of Added Initialization ---


    @router.post("/hybrid_search", response_model=HybridSearchResponse)
    async def hybrid_search(
        request_body: HybridSearchQuery, 
        request: fastapi.Request,
        userID: str = fastapi.Header(...)  # Required header
    ):
        """
        Combine responses from qachain and document_search using LLM.
        
        Args:
            request_body: Query with optional domain
            request: FastAPI request object
            userID: User ID from request header
            
        Returns:
            Combined response from both sources with individual results for comparison
        """
        logger.info(f"Hybrid search query: {request_body.query} for user: {userID}")

        # Check if services initialized correctly
        if not knowledge_search_service or not doc_search_service:
            logger.error("Search services are not initialized.")
            raise fastapi.HTTPException(status_code=503, detail="Search services are unavailable.")
            
        try:
            # --- MODIFICATION: Replaced placeholders with real calls ---
            
            logger.info("Running QA chain and Document Search in parallel...")
            # Run both search tasks concurrently
            qa_task = knowledge_search_service.qa_chain_response(request_body.query)
            doc_task = doc_search_service.invoke_chain(request_body.query, userID=userID)
            
            # Wait for both to finish
            results = await asyncio.gather(
                qa_task,
                doc_task,
                return_exceptions=True  # Don't let one failure stop the other
            )

            # --- Process QA Chain Result ---
            if isinstance(results[0], Exception):
                logger.error(f"QA chain task failed: {results[0]}")
                qachain_result = "Error: QA chain failed to produce a result."
            else:
                qachain_result = str(results[0])  # qa_chain_response returns a string

            # --- Process Document Search Result ---
            if isinstance(results[1], Exception):
                logger.error(f"Document search task failed: {results[1]}")
                doc_search_result = "Error: Document search failed to produce a result."
            elif isinstance(results[1], HTMLResponse):
                logger.warn("Document search returned HTML, not text.")
                doc_search_result = "Document search returned a non-textual HTML response."
            else:
                # Handle object (from document_search_response) or string
                doc_search_result = str(getattr(results[1], 'answer', results[1]))

            # --- End of MODIFICATION ---
            
            logger.info(f"QA Chain Response: {qachain_result}")
            logger.info(f"Document Search Response: {doc_search_result}")
            
            # Combine responses using LLM (Your unchanged function)
            combined_result = await combine_responses_with_llm(
                request_body.query,
                str(qachain_result), # Ensure it's a string
                str(doc_search_result) # Ensure it's a string
            )
            
            # Return using your defined Pydantic model
            return HybridSearchResponse(
                combined_result=combined_result,
                qachain_response=qachain_result,
                document_search_response=doc_search_result
            )
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {str(e)}")
            raise fastapi.HTTPException(status_code=500, detail=str(e))


    # --- UNCHANGED: Your function, exactly as written ---
    # (Note: This is inefficient as it re-initializes the client
    # on every call, but I am not changing it per your request.)
    async def combine_responses_with_llm(query: str, qachain_response: str, doc_search_response: str) -> str:
        """
        Use LLM to combine and synthesize responses from both sources.
        
        Args:
            query: Original user query
            qachain_response: Response from QA chain
            doc_search_response: Response from document search
            
        Returns:
            Combined and refined response
        """
        prompt = f"""
You are a technical support AI assistant specialized in synthesizing information from multiple sources.

User Query: {query}

Source 1 - Conversational AI Agent:
{qachain_response}

Source 2 - Knowledge Base Documents:
{doc_search_response}

Instructions:
1. Provide a concise, actionable answer (3-5 sentences maximum)
2. Merge complementary information from both sources
3. If sources conflict, prefer the most recent or authoritative information
4. Use bullet points only for step-by-step procedures
5. Avoid redundant explanations
6. Focus on practical solutions
7. If one source is significantly better, use that as primary and supplement with the other

Format your response as:
- Direct answer to the query
- Key action items (if applicable)
- Brief explanation (if needed)

Provide ONLY the synthesized response without meta-commentary:
"""
        
        try:
            from google.genai import Client
            from google.cloud import aiplatform
            import os
            
            project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "sab-dev-gen-ai-tech-6095")
            location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
            model_name = os.getenv("GOOGLE_GEMINI_MODEL", "gemini-2.0-flash-exp")
            
            aiplatform.init(project=project_id, location=location)
            client = Client(vertexai=True, project=project_id, location=location)
            
            response = await client.aio.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    top_p=0.9,
                    max_output_tokens=1024,
                )
            )
            
            combined_response = response.text.strip()
            logger.info(f"Combined LLM Response: {combined_response}")
            return combined_response
            
        except Exception as e:
            logger.error(f"Error combining responses with LLM: {str(e)}")
            return f"**Quick Answer:** {qachain_response[:200]}...\n\n**Additional Context:** {doc_search_response[:200]}..."

    return router
