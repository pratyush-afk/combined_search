import fastapi
import os  # <-- Moved to top
import textwrap  # <-- Added for prompt
from pydantic import BaseModel
from typing import Optional, Any
from google import genai  # <-- Changed to root import
from google.genai import types
from google.cloud import aiplatform  # <-- Moved to top
from utilities.custom_logging import log as logger


# --- Pydantic Models ---

class HybridSearchQuery(BaseModel):
    """Model for hybrid search query input."""
    query: str
    domain: Optional[str] = None


class HybridSearchResponse(BaseModel):
    """Model for hybrid search response."""
    combined_result: str
    # --- MODIFICATION ---
    # Added the two fields below to match your return statement
    qachain_response: str
    document_search_response: str


# --- Helper Function for LLM ---

async def combine_responses_with_llm(
    query: str, 
    qachain_response: str, 
    doc_search_response: str,
    client: genai.Client,  # <-- Pass the initialized client
    model_name: str         # <-- Pass the model name
) -> str:
    """
    Use LLM to combine and synthesize responses from both sources.
    
    Args:
        query: Original user query
        qachain_response: Response from QA chain
        doc_search_response: Response from document search
        client: Initialized GenAI client
        model_name: Name of the model to use
        
    Returns:
        Combined and refined response
    """
    prompt = textwrap.dedent(f"""
You are a technical support AI assistant specialized in synthesizing information from multiple sources.

User Query: {query}

Source 1 - Conversational AI Agent:
{qachain_response}

Source 2 - Knowledge Base Documents:
{doc_search_response}

Instructions:
1. Provide a concise, actionable answer (3-5 sentences maximum)
2. Merge complementary information from both sources
3. If sources conflict, prefer the most recent or authoritative information
4. Use bullet points only for step-by-step procedures
5. Avoid redundant explanations
6. Focus on practical solutions
7. If one source is significantly better, use that as primary and supplement with the other

Format your response as:
- Direct answer to the query
- Key action items (if applicable)
- Brief explanation (if needed)

Provide ONLY the synthesized response without meta-commentary:
""")
    
    try:
        response = await client.aio.models.generate_content(
            model=model_name,
            contents=prompt,
            generation_config=types.GenerateContentConfig(
                temperature=0.2,
                top_p=0.9,
                max_output_tokens=1024,
            )
        )
        
        combined_response = response.text.strip()
        logger.info(f"Combined LLM Response: {combined_response}")
        return combined_response
        
    except Exception as e:
        logger.error(f"Error combining responses with LLM: {str(e)}")
        # Fallback response if LLM fails
        return f"**Quick Answer:** {qachain_response[:200]}...\n\n**Additional Context:** {doc_search_response[:200]}..."


# --- Router Definition ---

def get_router() -> fastapi.APIRouter:
    """
    Create a router for hybrid search combining QA chain and document search.

    Returns:
        fastapi.APIRouter : Router for /hybrid_search
    """
    router = fastapi.APIRouter()

    # --- MODIFICATION ---
    # Initialize LLM client and variables here (at startup)
    try:
        project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "sab-dev-gen-ai-tech-6095")
        location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        model_name = os.getenv("GOOGLE_GEMINI_MODEL", "gemini-1.5-flash") # Using 1.5 flash as example
        
        aiplatform.init(project=project_id, location=location)
        # Initialize the client once and reuse it
        client = genai.Client(vertexai=True, project=project_id, location=location)
        
        logger.info(f"GenAI Client initialized for project: {project_id}, model: {model_name}")
        
    except Exception as e:
        logger.error(f"Failed to initialize GenAI Client: {e}")
        client = None # Handle potential init failure
        model_name = None

    @router.post("/hybrid_search", response_model=HybridSearchResponse)
    async def hybrid_search(
        request_body: HybridSearchQuery, 
        request: fastapi.Request,
        userID: str = fastapi.Header(...)  # Required header
    ):
        """
        Combine responses from qachain and document_search using LLM.
        
        Args:
            request_body: Query with optional domain
            request: FastAPI request object
            userID: User ID from request header
            
        Returns:
            Combined response from both sources with individual results for comparison
        """
        logger.info(f"Hybrid search query: {request_body.query} for user: {userID}")
        
        if not client:
            # Handle the case where client initialization failed at startup
            raise fastapi.HTTPException(status_code=503, detail="AI Service is unavailable")

        try:
            # --- TODO: Uncomment and implement these calls ---
            # You will need to get your 'qachain' and 'document_search'
            # functions/objects from your other files and call them here.
            
            # Example: from services.qa_service import qachain_service
            # Example: from services.doc_service import doc_search_service
            
            # qachain_result = await qachain_service.get_response(request_body.query, userID, request_body.domain)
            # doc_search_result = await doc_search_service.get_response(request_body.query, request_body.domain)
            
            # Using stubbed data as per your file:
            qachain_result = "QA Chain response: This is the conversational AI response to your query about login issues in Check In."
            doc_search_result = "Document search response: These are the relevant documents from the knowledge base about Check In login procedures."
            
            # ---------------------------------------------------
            
            logger.info(f"QA Chain Response: {qachain_result}")
            logger.info(f"Document Search Response: {doc_search_result}")
            
            # Combine responses using LLM
            combined_result = await combine_responses_with_llm(
                request_body.query,
                str(qachain_result),
                str(doc_search_result),
                client,      # Pass the initialized client
                model_name   # Pass the model name
            )
            
            # This return statement now matches the HybridSearchResponse model
            return HybridSearchResponse(
                combined_result=combined_result,
                qachain_response=qachain_result,
                document_search_response=doc_search_result
            )
            
        except Exception as e:
            logger.error(f"Error in hybrid search: {str(e)}")
            raise fastapi.HTTPException(status_code=500, detail=str(e))

    return router
